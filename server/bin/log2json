#!/usr/bin/env python
# -*- coding: utf8 -*-
"""
  log2json
  --------
  Transform raw log stream to JSON event stream

  usage: log2json [-h] [--sid SID] input output

  positional arguments:
    input       URI of raw input stream
    output      URI of output stream

  optional arguments:
    -h, --help  show this help message and exit
    --sid SID   set input socket identity

  :copyright: (c) 2012 by Ori Livneh <ori@wikimedia.org>
  :license: GNU General Public Licence 2.0 or later

"""
from __future__ import unicode_literals

import argparse
import logging
import sys

import jsonschema
import zmq

from eventlogging.schema import get_schema
from eventlogging.compat import json, items, parse_qsl, unquote
from eventlogging.stream import zmq_subscribe
from eventlogging.utils import ncsa_to_epoch, hash_value


META_SCHEMA_REV = 4891798

logging.basicConfig(level=logging.DEBUG, stream=sys.stderr)

parser = argparse.ArgumentParser(
    description='Transform raw log stream to JSON event stream')
parser.add_argument('input', help='URI of raw input stream')
parser.add_argument('output', help='URI of output stream')
parser.add_argument('--sid', help='set input socket identity')
args = parser.parse_args()

# Maps compressed query param name of auto fields to human-readable
# name. I hate this because it veers off the path of explicitly modelled
# data and back toward the land of ad-hoc customizations. But the
# expansion of the max URL length to 1024 bytes will allow us to send
# raw URL-encoded JSON instead of key=val params, which will resolve
# some of this.
meta_readable = {
    '_rv': 'revision',
    '_id': 'schema',
    '_ok': 'valid',
    '_db': 'site'
}


# Maps JSON schema types to a function that will convert a string to
# that type. Ugly. Also going away when we move to URL-encoded JSON.
casters = {
    'integer': int,
    'array': lambda x: x.split(','),
    'boolean': lambda x: x.lower() == 'true',
    'null': lambda x: None,
    'number': lambda x: float(x) if '.' in x else int(x),
    'string': lambda x: x
}


def typecast(obj, schema):
    """
    Optimistically attempt to cast the value of each property to the
    type specified for that property by the schema.
    """
    properties = schema.get('properties', {})
    types = {k: v.get('type') for k, v in items(properties)}
    return {k: casters[types.get(k, 'string')](v) for k, v in items(obj)}


def decode_event(q):
    """
    Hack: dispatch appropriate decoder through content sniffing.
    """
    q = q.strip('?;')
    if q.startswith('%7B'):
        return decode_json_event(q)
    return decode_qs_event(q)


def decode_json_event(q):
    """
    Decodes a query string containing a single URL-encoded
    JSON-serialized event and validates it.
    """
    e = json.loads(unquote(q))
    meta = {k.lstrip('_'): v for k, v in items(e.pop('meta'))}
    schema = get_schema(meta['revision'])
    jsonschema.validate(e, schema)
    e['meta'] = meta
    return e


def decode_qs_event(q):
    """
    Decodes a query string generated by EventLogging to a Python object
    by matching it with a schema and validating it accordingly.
    """
    q = dict(parse_qsl(q.strip('?;')))
    meta = {}
    e = {}
    for k, v in items(q):
        if k.startswith('_'):
            meta[k] = v
        else:
            e[k] = v

    metaschema = get_schema(META_SCHEMA_REV)
    meta = typecast(meta, metaschema)

    schema = get_schema(meta['_rv'])
    e = typecast(e, schema)
    jsonschema.validate(e, schema)

    e['meta'] = {meta_readable.get(k, k): v for k, v in items(meta)}
    return e


def parse_bits_line(line):
    """Parse a log line emitted by varnishncsa on the bits hosts."""
    try:
        q, origin, seq_id, timestamp, client_ip = line.split()
        e = decode_event(q)
    except (ValueError, KeyError, jsonschema.ValidationError):
        logging.exception('Unable to decode: %s', line)
        return None

    e['meta'].update({
        'truncated': not q.endswith(';'),
        'origin': origin.split('.', 1)[0],
        'seqId': int(seq_id),
        'timestamp': ncsa_to_epoch(timestamp),
        'clientIp': hash_value(client_ip)
    })

    return e


if __name__ == '__main__':
    context = zmq.Context.instance()
    pub = context.socket(zmq.PUB)
    pub.bind(args.output)

    logging.info('Publishing JSON events on %s.', args.output)

    for raw_event in zmq_subscribe(args.input, sid=args.sid):
        event = parse_bits_line(raw_event)
        if event is not None:
            # We can't use pyzmq's Socket.send_json because it doesn't
            # send a trailing newline and our stream is line-oriented.
            pub.send_unicode(json.dumps(event) + '\n')
